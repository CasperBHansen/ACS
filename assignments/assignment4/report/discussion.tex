%==============================================================================%
% DISCUSSION                                                                   %
%==============================================================================%

\section{Discussion on the Performance Measurements}

\subsection{Setup}
The implementation of the tests is a pretty direct implementation of the
specifications of the test. The interesting part is how we ensure the test
show anything meaningful. In particular, we generate one big set of books and
only include a subset of these in the bookstore from the beginning. As a
design choice, we have abstracted out generation of books to the
{\tt BookSetGenerator} which we only make a single instance of passed by
reference to the workers. We feel this should in fact be singleton. This has
not been done for the {\tt WorkloadConfiguration} even though we feel the same
applies. Note also that we have a remaining error which causes certain ISBN
numbers to be duplicated. We will discuss the potential effects on the test
results separately.

\subsection{Plots}
\dots

\subsection{Reliability}
In the interest of profit, from a business perspective, the metric employed
makes sense.

% TODO: test on system with low performance, and on one with high performance
%       to argue that the reliability is sound.

We do, however, believe that the metric suffers some loss of what we would
consider critical data. While the metric is oriented toward measuring supply
and demand, it fails to capture ---perhaps infrequent, but highly important---
instances in which the system does not perform well. In order to do so, one
could alter the metric slightly to be more versatile in its description of the
system, by way of a distribution or histogram --- that is, the distribution of
outcomes per time interval. This way we can make more solid arguments about the
system and its performance overall, rather than mere successes per time interval.
