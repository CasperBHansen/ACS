%==============================================================================%
% DISCUSSION                                                                   %
%==============================================================================%

\section{Discussion on Replication Mechanism}

\subsection{Implementation and strategy}
We employ the provided thread pool in
{\tt com.acertainbookstore.business.CertainBookStoreReplicator} to submit
{\tt com.acertainbookstore.business.CertainBookStoreReplicationTask}s concurrently,
which allows for overlapping replication across slaves, which in turn hides
latency.

We have chosen a simple load-balancing strategy, in which is handled in two
places; in {\tt sendToAvailableReplica} and {\tt getReplicaAddress}. The latter
is used as a utility method for the former. In our design
{\tt sendToAvailableReplica} serves as a wrapper around {\tt SendAndRecv}, such
that we one and only one place where we control failure logic. In this wrapper
we decide where to send the exchange, firstly based on the message tag, such
that reads are delegated to slaves, and writes are delegated to the master. In
the event that a slave fails, it will mark it as faulty, and retry on a different
slave. If all slaves have failed, as a last resort, we will try to perform the
action on the master server.

We made a rather radical decision to extend the API to facilitate a new type of
exception, which we felt was the right choice for handling fail-stop. The old
{\tt BookStoreException}s still serve their purpose, while the newly introced
{\tt NetworkException} serves as an indicator as to communication failures, and
allows us to catch those specific occurrences, such that we might handle them
appropriately. Because we have wrapped all proxy exchanges, it is the only
place in which we need to handle it, and we do so as described above.

Added parameters?

\subsection{Advantages and disadvantages}
The replicated implementation first and foremost, increases concurrency on
read by using load balancing to distribute reads evenly among the available
servers (Master and Slaves). If the lack of concurrency was causing
bottlenecks, this should theoretically (needs testing) increase throughput and
reduces latency. Since all servers needs to fail simultaneously for the read
services to be unavailable, this is an increase in availability. The cost is
with regards to writes. Since each write operation needs to be replicated,
this presents some overhead due to implementation complexity and since each
slave must contacted via http (or some other network protocol), this adds
extra latency on write operations. A potential bottleneck is how we need to
wait for slaves to be updated before carrying out reads. It should be possible
to implement something like a 3 phase commit in this regard.

\subsection{Proxy and client failures}
Two possible ways are immediately apparent. Either proxies communicate with
each other to establish what the current state of the server should be. We
think this is still an error-prone strategy since we do not know if this state
information is broadcasted early enough such that the client has not yet
established contact with another proxy. Correctness would probably come at an
unacceptable cost to concurrency. Alternatively, the client stores information
(perhaps in form of a handle), storing information to facilitate possibility
of verification at proxy that the server is updated.

\subsection{Network partition}
We would lose all guarantees that the data is up-to-date, since the proxies
forward read requests to slaves out of contact with the master. The timestamps
would not be a good indicator of data correctness since the slave servers time
is independent of the contact to the master.
